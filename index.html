<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/html">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  <title>XLLM Workshop @ ACL 2025</title>
  <meta name="description" content="">
  <meta name="keywords" content="">

  <!-- Favicons -->
  <link href="assets/img/favicon.png" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Fonts -->
  <link href="https://fonts.googleapis.com" rel="preconnect">
  <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300;0,400;0,500;0,600;0,700;0,800;1,300;1,400;1,500;1,600;1,700;1,800&family=Montserrat:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&family=Poppins:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Main CSS File -->
  <link href="assets/css/main.css" rel="stylesheet">

  <!-- =======================================================
  * Template Name: Append
  * Template URL: https://bootstrapmade.com/append-bootstrap-website-template/
  * Updated: Aug 07 2024 with Bootstrap v5.3.3
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body class="index-page">

  <header id="header" class="header d-flex align-items-center fixed-top">
    <div class="container-fluid position-relative d-flex align-items-center justify-content-between" style="height: 20px">

      <a href="./" class="logo d-flex align-items-center me-auto me-xl-0">
<!--           style="margin-left: 200px"-->
        <!-- Uncomment the line below if you also wish to use an image logo -->
        <!-- <img src="assets/img/logo.png" alt=""> -->
<!--        <h1 class="sitename">XLLM 2025</h1>-->
      </a>

      <nav id="navmenu" class="navmenu">
<!--           style="margin-left: -200px"-->
        <ul>
          <li><a href="./" class="active">Overview</a></li>
          <li><a href="./#date">Important Dates</a></li>
          <li><a href="./#call">Call for Papers</a></li>
          <li><a href="./#keynote">Keynote Speakers</a></li>
          <li><a href="./#schedule">Schedule</a></li>
          <li><a href="./#task">Shared Tasks</a></li>
          <li><a href="./#organizer">Organizers</a></li>
          <li><a href="./#committee">Program Committee</a></li>
          <li><a href="./#contact">Contact</a></li>
<!--          <li class="dropdown"><a href="#"><span>Previous Versions</span> <i class="bi bi-chevron-down toggle-dropdown"></i></a>-->
<!--            <ul>-->
<!--              <li><a href="#">Dropdown 1</a></li>-->
<!--              <li><a href="#">Dropdown 2</a></li>-->
<!--              <li><a href="#">Dropdown 3</a></li>-->
<!--              <li><a href="#">Dropdown 4</a></li>-->
<!--            </ul>-->
<!--          </li>-->
        </ul>
        <i class="mobile-nav-toggle d-xl-none bi bi-list"></i>
      </nav>

      <div></div>

<!--      <a class="btn-getstarted" href="./#about">Get Started</a>-->

    </div>
  </header>

  <main class="main">

    <!-- Hero Section -->
    <section id="hero" class="hero section dark-background">

      <img src="assets/img/Vienna-crop2.jpeg" alt="" data-aos="fade-in">

      <div class="container">
        <div class="row">
          <div class="col-lg-10" style="margin-left: 50px;margin-bottom: -50px;">
            <h1 data-aos="fade-up" data-aos-delay="100">XLLM @ <a href="https://2025.aclweb.org/">ACL 2025</a></h1>
            <br>
            <h3 data-aos="fade-up" data-aos-delay="100">The 1<sup>st</sup> Joint Workshop on Large Language Models and Structure Modeling</h3>
            <br>
            <p data-aos="fade-up" data-aos-delay="200">Vienna, Austria</p>
            <p data-aos="fade-up" data-aos-delay="200">August 1st, 2025 (All day event)</p>
          </div>
        </div>
      </div>

    </section><!-- /Hero Section -->





    <!-- About Section -->
    <section id="about" class="about section light-background">

      <div class="container" data-aos="fade-up" data-aos-delay="100" style="    margin-top: -30px;">
        <div class="row align-items-xl-center gy-2">

          <div class="content">
            <h3 style=" margin-bottom: 30px;">Workshop Introduction</h3>
<!--            <h2>Ducimus rerum libero reprehenderit cumque</h2>-->
            <p align="justify">Language structure modeling has long been a crucial subfield of natural language processing (NLP) that entails understanding the underlying semantic or syntactic structure of language and texts. Language structures can broadly range from low-level morphological/syntactic types (e.g., dependency structures and phrasal constituent structures) to high-level discourse/semantic structures (e.g., semantic parsing, semantic role labeling, abstract meaning representation), and can even extend to more NLP applications, multi-lingual and multi-modal scenarios in a broader sense, such as information extraction and structured sentiment analysis, etc. In previous days, modeling, inferring, and learning about linguistic structures constituted an indispensable component in many NLP systems and were the key focus of a large proportion of NLP research.</p>

            <p align="justify">The methodologies and paradigms concerning language structure modeling have always changed dramatically since each deep learning revolution started around a decade ago. In the last two to three years, Large Language Models (LLMs) have emerged, demonstrating unprecedented language understanding and generalization capabilities in effectively addressing a wide range of tasks. This raises a critical question:
              <em style="color: orangered">Is NLP structure modeling still worth exploring in the LLM era? Do the methods and tasks before LLMs still hold value?</em></p>

            <p align="justify">On the one hand, we wonder whether previous NLP structure modeling tasks, such as those concerning morphological/syntactic/semantic/discourse structures and high-level structure-aware applications, can achieve <b>even stronger task performance</b> with the powerful capabilities of LLMs.</p>

            <p align="justify">On the other hand, we are also considering whether it is still necessary to model the underlying structures of language, given that large-scale pretraining on the surface form alone can endow LLMs with extraordinarily powerful language capabilities. <b>In particular, can language structure modeling be beneficial for improving or understanding LLMs?</b></p>

            <p align="justify">Thus, this 1st Joint Workshop on Large Language Models and Structure Modeling (XLLM 2025) at ACL 2025 aims to encourage discussions and highlight methods for language structure modeling in the era of LLMs. Specifically, we will explore two main directions:
              <b>LLM for Structure Modeling</b> (LLM4X) and <b>Structure Modeling for LLM</b> (X4LLM).</p>

<!--            <a href="#" class="read-more"><span>Read More</span><i class="bi bi-arrow-right"></i></a>-->
          </div>


          <h2 class="title is-3" style="margin-top: 40px;">🔔News</h2>

          <div class="content has-text-justified">
            <p>
              <b>🔥 [2025-06-15]: Our XLLM workshop will be held in <b style="color: #E84545">August 1st, 2025</b>, as a whole day event!</b><br>
              <b>🔥 [2025-06-01]: All the decisions of submissions for regular/ARR extended abstract papers, and the shared task papers are sent to all authors!</b><br>
              <b>🔥 [2025-05-01]: We are working hard in finalizing the decisions of regular workshop submissions, and the notifications will be sent by <b style="color: #E84545">May 5, 2025 (AoE)</b>!</b><br>
              <b>🔥 [2025-04-27]: Openreview entry for <b>Non-archival Extended Abstract Submission</b> is ready at <a href="https://openreview.net/group?id=aclweb.org/ACL/2025/Workshop/XLLM_Extended_Abstract">here</a>, submit paper as soon!</b><br>
              <b>🔥 [2025-03-31]: We again extend the workshop direct submission deadline from Mar 31 to <b style="color: #E84545">Apr 6, 2025 (AoE)</b>! Welcome submissions and don't miss the boat!</b><br>
              <b>🔥 [2025-03-14]: We extend the workshop direct submission deadline from Mar 18 to <b style="color: #E84545">Mar 31, 2025</b>! Welcome submissions!</b><br>
              <b>🔥 [2025-03-12]: We extend all the shared task deadlines all to <b style="color: #E84545">Apr 6, 2025</b>! Welcome participation!</b><br>
              <b>🔥 [2025-02-10]: All our data of shared tasks are released! Welcome participation!</b><br>
              <b>🔥 [2025-02-05]: Second round of call for paper and participation is out! Welcome submissions!</b><br>
              <b>🔥 [2024-12-25]: First call for paper is out! Welcome submissions!</b><br>
<!--              <b>🔥 Our tutorial is about to start, at <u>room Summit 446</u> for in-person attendance!</b><br>-->
<!--              <b>🔥 Also you may want to join our online Tutorial via this <a href="#"><s>Zoom link</s></a>!</b>-->
            </p>
            <br>
         </div>
<!--          <div class="col-xl-7">-->
<!--            <div class="row gy-4 icon-boxes">-->

<!--              <div class="col-md-6" data-aos="fade-up" data-aos-delay="200">-->
<!--                <div class="icon-box">-->
<!--                  <i class="bi bi-buildings"></i>-->
<!--                  <h3>Eius provident</h3>-->
<!--                  <p>Magni repellendus vel ullam hic officia accusantium ipsa dolor omnis dolor voluptatem</p>-->
<!--                </div>-->
<!--              </div> &lt;!&ndash; End Icon Box &ndash;&gt;-->

<!--              <div class="col-md-6" data-aos="fade-up" data-aos-delay="300">-->
<!--                <div class="icon-box">-->
<!--                  <i class="bi bi-clipboard-pulse"></i>-->
<!--                  <h3>Rerum aperiam</h3>-->
<!--                  <p>Autem saepe animi et aut aspernatur culpa facere. Rerum saepe rerum voluptates quia</p>-->
<!--                </div>-->
<!--              </div> &lt;!&ndash; End Icon Box &ndash;&gt;-->

<!--              <div class="col-md-6" data-aos="fade-up" data-aos-delay="400">-->
<!--                <div class="icon-box">-->
<!--                  <i class="bi bi-command"></i>-->
<!--                  <h3>Veniam omnis</h3>-->
<!--                  <p>Omnis perferendis molestias culpa sed. Recusandae quas possimus. Quod consequatur corrupti</p>-->
<!--                </div>-->
<!--              </div> &lt;!&ndash; End Icon Box &ndash;&gt;-->

<!--              <div class="col-md-6" data-aos="fade-up" data-aos-delay="500">-->
<!--                <div class="icon-box">-->
<!--                  <i class="bi bi-graph-up-arrow"></i>-->
<!--                  <h3>Delares sapiente</h3>-->
<!--                  <p>Sint et dolor voluptas minus possimus nostrum. Reiciendis commodi eligendi omnis quideme lorenda</p>-->
<!--                </div>-->
<!--              </div> &lt;!&ndash; End Icon Box &ndash;&gt;-->

<!--            </div>-->
<!--          </div>-->

        </div>
      </div>

    </section><!-- /About Section -->

    <!-- Stats Section -->
<!--    <section id="stats" class="stats section dark-background">-->

<!--      <img src="assets/img/stats-bg.jpg" alt="" data-aos="fade-in">-->

<!--      <div class="container position-relative" data-aos="fade-up" data-aos-delay="100">-->

<!--        <div class="row gy-4">-->

<!--          <div class="col-lg-3 col-md-6">-->
<!--            <div class="stats-item text-center w-100 h-100">-->
<!--              <span data-purecounter-start="0" data-purecounter-end="232" data-purecounter-duration="1" class="purecounter"></span>-->
<!--              <p>Clients</p>-->
<!--            </div>-->
<!--          </div>&lt;!&ndash; End Stats Item &ndash;&gt;-->

<!--          <div class="col-lg-3 col-md-6">-->
<!--            <div class="stats-item text-center w-100 h-100">-->
<!--              <span data-purecounter-start="0" data-purecounter-end="521" data-purecounter-duration="1" class="purecounter"></span>-->
<!--              <p>Projects</p>-->
<!--            </div>-->
<!--          </div>&lt;!&ndash; End Stats Item &ndash;&gt;-->

<!--          <div class="col-lg-3 col-md-6">-->
<!--            <div class="stats-item text-center w-100 h-100">-->
<!--              <span data-purecounter-start="0" data-purecounter-end="1453" data-purecounter-duration="1" class="purecounter"></span>-->
<!--              <p>Hours Of Support</p>-->
<!--            </div>-->
<!--          </div>&lt;!&ndash; End Stats Item &ndash;&gt;-->

<!--          <div class="col-lg-3 col-md-6">-->
<!--            <div class="stats-item text-center w-100 h-100">-->
<!--              <span data-purecounter-start="0" data-purecounter-end="32" data-purecounter-duration="1" class="purecounter"></span>-->
<!--              <p>Workers</p>-->
<!--            </div>-->
<!--          </div>&lt;!&ndash; End Stats Item &ndash;&gt;-->

<!--        </div>-->

<!--      </div>-->

<!--    </section>&lt;!&ndash; /Stats Section &ndash;&gt;-->

    <!-- Services Section -->
    <section id="date" class="date section">

      <!-- Section Title -->
      <div class="container section-title" data-aos="fade-up">
        <h2>Important Dates</h2>
<!--        <p>Necessitatibus eius consequatur ex aliquid fuga eum quidem sint consectetur velit</p>-->
      </div><!-- End Section Title -->


      <div class="container">
          <div class="gy-2 row">
            <p>All deadlines are specified in AoE (Anywhere on Earth).
            </p>

            <b style="color: #0d6efd">[Workshop Timeline]
            </b>
              <div class="col-lg-8 col-12">
                  <table class="table table-borderless">
                      <tbody>
<!--                          <tr>-->
<!--                              <th>December 25, 2024</th>-->
<!--                              <td>First call for workshop papers</td>-->
<!--                          </tr>-->

<!--                          <tr>-->
<!--                              <th>January 24, 2025</th>-->
<!--                              <td>Second call for workshop papers</td>-->
<!--                          </tr>-->
<!--                          <tr>-->
<!--                              <th>February 10, 2025</th>-->
<!--                              <td>Training data and participant instruction release for all shared tasks</td>-->
<!--                          </tr>-->
<!--                          <tr>-->
<!--                              <th>February 24, 2025</th>-->
<!--                              <td>Third call for workshop papers</td>-->
<!--                          </tr>-->
                          <tr>
<!--                              <th style="color: red;"><del style="color: black;">May 17</del> May 31, 2025</th>-->
                              <th><s>March 18, 2025</s> <s><b style="color: #E84545">March 31</b></s> <s><b style="color: #E84545">April 6</b></s></th>
                              <td>Regular workshop paper: direct submission deadline</td>
                          </tr>

                          <tr>
                              <th><s>April 20, 2025</s></th>
                              <td>Regular workshop paper: ARR pre-reviewed paper commitment deadline</td>
                          </tr>

<!--                          <tr>
                              <th >April 5, 2025</th>
                              <td>Notification of all shared tasks</td>
                          </tr>-->

                          <tr>
                              <th><s>April 30, 2025</s> <s><b style="color: #E84545">May 5</b></s></th>
                              <td>Regular workshop paper: acceptance notification</td>
                          </tr>
						  
                          <tr>
                              <th><s>May 16, 2025</s> <s><b style="color: #E84545">May 25</b></s></th>
                              <td>Regular workshop paper: camera-ready deadline</td>

                          <tr>
                              <th><s>May 30, 2025</s></th>
                              <td>Non-archival extended abstract: direct submission deadline</td>
                          </tr>
                          <tr>
                              <th><s>June 7, 2025</s></th>
                              <td>Non-archival extended abstract: acceptance notification</td>
                          </tr>
                          </tr>

<!--                          <tr>-->
<!--                              <th>June 30, 2025</th>-->
<!--                              <td>Proceedings due (hard deadline)</td>-->
<!--                          </tr>-->


<!--                           <tr>
                              <th>July 7, 2025</th>
                              <td>Pre-recorded video due (hard deadline)</td>
                          </tr> -->


                          <tr>
                              <th>August 1, 2025</th>
                              <td>Workshop dates (TBD)</td>
                          </tr>


                      </tbody>
                  </table>
              </div>


            <b style="color: #07b24b;margin-top: 10px">[Shared Task Timeline]
            </b>
              <div class="col-lg-8 col-12">
                  <table class="table table-borderless">
                      <tbody>
                          <tr>
                              <th><s>February 10, 2025</s></th>
                              <td>Training data and participant instruction release for all shared tasks</td>
                          </tr>

                          <tr>
                              <th ><s>March 30, 2025</s> <s><b style="color: #E84545">Apr 6, 2025</b></s></th>
                              <td>Evaluation deadline for all shared tasks</td>
                          </tr>
                          <tr>
                              <th><s>April 5, 2025</s> <s><b style="color: #E84545">Apr 12, 2025</b></s></th>
                              <td>Notification of all shared tasks</td>
                          </tr>
                          <tr>
                              <th><s>April 20, 2025</s> <s><b style="color: #E84545">Apr 25, 2025</b></s></th>
                              <td>Shared-task paper submission deadline</td>
                          </tr>

                          <tr>
                              <th><s>April 30, 2025</s></th>
                              <td>Shared-task paper acceptance notification</td>
                          </tr>
                          <tr>
                              <th><s>May 16, 2025</s></th>
                              <td>Shared-task paper camera-ready deadline</td>
                          </tr>

                      </tbody>
                  </table>
              </div>

          </div>
      </div>




    </section><!-- /Services Section -->

    <!-- Features Section -->
    <section id="call" class="call section">

      <!-- Section Title -->
      <div class="container section-title" data-aos="fade-up">
        <h2>Call for Papers</h2>
<!--        <p>Necessitatibus eius consequatur ex aliquid fuga eum quidem sint consectetur velit</p>-->
      </div><!-- End Section Title -->

      <div class="container">

        <h3 style="font-weight: 700; font-size: 32px;">Topics</h3>
        <div class="row gy-4">
          <p style="margin-top: 60px;margin-bottom: -10px">We welcome paper submissions on all topics related to structure modeling under LLMs, including but not limited to:
          </p>
          <ul style="padding-left: 40px;line-height: 1.8;">
            <li><b style="font-size: large">LLM for Structure Modeling (LLM4X)</b></li>
            <ul>
                <li><b>Low-level Syntactic Parsing and Methods</b></li>
                    <ul>
                        <li>Morphological Parsing</li>
                        <li>Dependency Parsing/Constituency Parsing</li>
                        <li>Low-resource/Cross-lingual Syntactic Parsing</li>
                        <li>Head-driven Phrase Structure Grammar Parsing</li>
                        <li>Unsupervised Grammar Induction</li>
                        <li>Cross-modal Parsing/Vision-Language Grammar Induction</li>
                    </ul>

                <li><b>High-level Semantic Parsing and Methods</b></li>
                    <ul>
                        <li>Semantic Dependency Parsing</li>
                        <li>Frame Parsing</li>
                        <li>Semantic Role Labeling</li>
                        <li>Abstract Meaning Representation</li>
                        <li>Uniform Meaning Representation</li>
                        <li>Universal Decompositional Semantic Parsing</li>
                        <li>Universal Conceptual Cognitive Annotation</li>
                        <li>Rhetorical Structure Theory (RST) Parsing</li>
                        <li>Conversation Discourse Parsing</li>
                        <li>Low-resource/Cross-lingual Semantic Parsing</li>
                    </ul>

                <li><b>Broader Structure-aware Applications and Methods</b></li>
                    <ul>
                        <li>Information Extraction (IE): NER, RE, EE</li>
                        <li>Structured Sentiment Analysis (SSA), Aspect-based Sentiment Analysis (ABSA)</li>
                        <li>Low-resource/Cross-lingual IE/SSA/ABSA/</li>
                        <li>Cross-modal IE/SSA/ABSA/</li>
                        <li>Text-to-SQL</li>
                        <li>Table Parsing</li>
                        <li>Document Parsing</li>
                        <li>Scene Graph Parsing</li>
                        <li>Universal Structure Parsing/Modeling</li>
                        <li>Human-centered Parsing with LLM</li>
                        <li>Robustness Analysis of LLM-based Parsing</li>
                    </ul>

            </ul>

            <li style="margin-top: 20px"><b  style="font-size: large">Structure Modeling for LLM (X4LLM)</b></li>
            <ul>
                <li>Linguistic and/or mathematical arguments for or against the utility of linguistic structures in language models</li>
                <li>Empirical studies of the utility of linguistic structures in language models</li>
                <li>Integration of various types of linguistic structures into transformers or other architectures underlying modern language models</li>
                <li>Incorporation of linguistic structures and representations as additional input or output in language modeling</li>
                <li>Incorporation of training signals from linguistic structures in language model pre-training and post-training</li>
                <li>Language model prompting with linguistic rules and structural information</li>
                <li>Analyses and interpretation of transformers and language models through the lens of linguistic structures</li>
            </ul>
          </ul>


        </div>


      <div class="container" style="margin-top: 40px">

        <h3 style="font-weight: 700; font-size: 32px;">Paper Submission Information</h3>
        <div class="row gy-4">
          <p style="margin-top: 40px;margin-bottom: -10px">We welcome two types of papers: regular papers and non-archival extended abstracts.
              All submissions must follow <a href="https://aclrollingreview.org/cfp#paper-submission-information">the format requirement of ACL/ARR</a>, and made through OpenReview.
          </p>
          <ul style="padding-left: 40px;">
            <li><b>Regular workshop papers:</b>
            <p style="text-align: justify">Authors can submit papers up to 8 pages, with unlimited pages for references. Authors may submit up to 100 MB of supplementary materials separately and their code for reproducibility. All submissions undergo a double-blind single-track review.
              We will set <b style="color: #dc3545">Best Paper Award(s)</b>, which will be given based on nomination by the reviewers.
              Accepted papers will be presented as posters with the possibility of oral presentations, and will be included in the workshop proceedings.</p>
            </li>

            <li style="margin-top: 10px"><b>Non-archival extended abstracts:</b>
            <p style="text-align: justify">Cross-submissions are welcome.
              Authors can submit extended abstracts <i>up to 2 pages</i>, with unlimited pages for references. An extended abstract may report on work in progress or work that has already appeared in or been accepted by another venue within two years before the workshop. It does not need to be anonymized, but should state explicitly where it was originally accepted or published. Accepted extended abstracts will be presented as posters and will not be included in the workshop proceedings.</p></li>

          </ul>

          <p style="margin-top: -20px;text-align: justify">In addition to papers submitted directly to the workshop, which will be reviewed by our Programme Committee, we also accept papers reviewed through ACL Rolling Review and committed to the workshop. Please check the relevant dates for each type of submission.
          </p>


          <div class="container" data-aos="fade-up" style="
            align-content: flex-start;
            display: flex;
            flex-direction: row;
            flex-wrap: nowrap;
            justify-content: center;">
            <div >

            <div class="row gy-2" style="
              display: flex;
              justify-content: space-between;
              flex-wrap: nowrap;
              flex-direction: row;">
                <blockquote style="width: fit-content;margin-right: 10px;">
                  <button type="button" class="btn  btn-outline-dark disabled "><a href="https://openreview.net/group?id=aclweb.org/ACL/2025/Workshop/XLLM"><b class="text-danger">Entry for Regular Submission</b></a></button>
                </blockquote>


                <blockquote style="width: fit-content;margin-right: 10px;">
                  <button type="button" class="btn  btn-outline-dark disabled "><a href="https://openreview.net/group?id=aclweb.org/ACL/2025/Workshop/XLLM_Extended_Abstract"><b class="text-danger">Entry for Non-archival Extended Abstract Submission</b></a></button>
                </blockquote>


                <blockquote>
                  <button type="button" class="btn  btn-outline-dark disabled "><a href="https://openreview.net/group?id=aclweb.org/ACL/2025/Workshop/XLLM_ARR_Commitment"><b class="text-danger">Entry for ARR Pre-reviewed Commitment</b></a></button>
                </blockquote>

            </div>
            </div>
          </div>

        </div>


<!--        <div class="row gy-4 align-items-center features-item">-->
<!--          <div class="col-lg-5 order-2 order-lg-1" data-aos="fade-up" data-aos-delay="200">-->
<!--            <h3>Corporis temporibus maiores provident</h3>-->
<!--            <p>-->
<!--              Ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate-->
<!--              velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident.-->
<!--            </p>-->
<!--            <a href="#" class="btn btn-get-started">Get Started</a>-->
<!--          </div>-->
<!--          <div class="col-lg-7 order-1 order-lg-2 d-flex align-items-center" data-aos="zoom-out" data-aos-delay="100">-->
<!--            <div class="image-stack">-->
<!--              <img src="assets/img/features-light-1.jpg" alt="" class="stack-front">-->
<!--              <img src="assets/img/features-light-2.jpg" alt="" class="stack-back">-->
<!--            </div>-->
<!--          </div>-->
<!--        </div>&lt;!&ndash; Features Item &ndash;&gt;-->

<!--        <div class="row gy-4 align-items-stretch justify-content-between features-item ">-->
<!--          <div class="col-lg-6 d-flex align-items-center features-img-bg" data-aos="zoom-out">-->
<!--            <img src="assets/img/features-light-3.jpg" class="img-fluid" alt="">-->
<!--          </div>-->
<!--          <div class="col-lg-5 d-flex justify-content-center flex-column" data-aos="fade-up">-->
<!--            <h3>Sunt consequatur ad ut est nulla</h3>-->
<!--            <p>Cupiditate placeat cupiditate placeat est ipsam culpa. Delectus quia minima quod. Sunt saepe odit aut quia voluptatem hic voluptas dolor doloremque.</p>-->
<!--            <ul>-->
<!--              <li><i class="bi bi-check"></i> <span>Ullamco laboris nisi ut aliquip ex ea commodo consequat.</span></li>-->
<!--              <li><i class="bi bi-check"></i><span> Duis aute irure dolor in reprehenderit in voluptate velit.</span></li>-->
<!--              <li><i class="bi bi-check"></i> <span>Facilis ut et voluptatem aperiam. Autem soluta ad fugiat</span>.</li>-->
<!--            </ul>-->
<!--            <a href="#" class="btn btn-get-started align-self-start">Get Started</a>-->
<!--          </div>-->
<!--        </div>&lt;!&ndash; Features Item &ndash;&gt;-->


	      
      </div>

    </section><!-- /Features Section -->















    <!-- Team Section -->
    <section id="keynote" class="keynote section">

      <!-- Section Title -->
      <div class="container section-title" data-aos="fade-up">
        <h2>Invited Keynote Speakers</h2>
<!--        <p>Necessitatibus eius consequatur ex aliquid fuga eum quidem sint consectetur velit</p>-->
      </div><!-- End Section Title -->


<!--       <div class="container">
          <p style="color: red;font-size: large;font-weight: bold;">- TBD -</p>
      </div> -->




      <div class="container">

          <div class="row py-4 px-lg-3 px-0">
              <div class="col-12 col-md-4 col-lg-2 mx-lg-2 order-first" style="display: flex;flex-direction: row;flex-wrap: nowrap;align-content: flex-start;justify-content: center;">
                  <div style="display: flex;justify-content: center;flex-direction: column;">
                      <a href="https://web.science.mq.edu.au/~mjohnson/"><img src="./assets/img/speaker/Mark-Johnson.jpg" class="d-block m-2 rounded card-img-top" loading="lazy" style="width:250px;"></a>
                  </div>
              </div>
              <div class="col mx-5">
                  <a href="https://web.science.mq.edu.au/~mjohnson/"><h4 class="text-center text-md-start">Mark Johnson</h4></a>
                  <p class="text-center text-md-start"><small>Professor at Macquarie University</small></p>
                  <p style="text-align: justify"><b>Bio:</b>Mark Johnson is a Professor of Language Science (CORE) in the School of Computing at Macquarie University. He is also the Chief AI Scientist, Oracle Digital Assistant at Oracle Corporation, where he develops chatbots and digital assistants. The Oracle Digital Assistant division develops novel deep learning models to power the next generation of Conversational AI using semantic parsing.
                Mark Johnson has worked on a wide range of topics in computational linguistics, but his main area of research is natural language understanding, especially syntactic parsing and semantic analysis, and their applications to text and speech processing.
                  </p>
                  <p><b>Title:</b> <u class="fw-normal">The Changing Roles of (Linguistic) Structure in Computational Linguistics </u></p>
                  <p style="text-align: justify"><b>Abstract:</b> 
			  This talk describes the various roles that linguistic theory and structure have played in computational linguistics, and speculates about the role that they may play in the future.  The closest relationship between linguistics and computational linguistics was probably with the Unification Grammars introduced in the 1980s, where the goal was to develop a computational model that implemented the linguistic theory.  This close relationship proved impractical for scientific and sociological reasons that I’ll describe, and since then the relationship has steadily weakened.  I argue that the huge training data and long context windows of Deep Learning models makes it unnecessary to incorporate any specific linguistically-inspired parsing architecture into such models.  While there are deep scientific questions about how LLMs “understand” human languages, their linguistic ability is sufficiently good for most practical tasks. Quite reasonably most current research focuses on the information content of the language LLMs generate, such as reducing hallucinations and improving instruction-following.  Thus it seems the main opportunities for linguistics to contribute the modern computational linguistics are in model evaluation and explainability.  
                  </p>
              </div>
          </div>
      </div>



      <div class="container" style="margin-top: 40px">

          <div class="row py-4 px-lg-3 px-0">
              <div class="col-12 col-md-4 col-lg-2 mx-lg-2 order-first" style="display: flex;flex-direction: row;flex-wrap: nowrap;align-content: flex-start;justify-content: center;">
                  <div style="display: flex;justify-content: center;flex-direction: column;">
                      <a href="https://mariannaapi.github.io/"><img src="./assets/img/speaker/Jan-Hajič.jpg" class="d-block m-2 rounded card-img-top" loading="lazy" style="width:250px;"></a>
                  </div>
              </div>
              <div class="col mx-5">
                  <a href="https://mariannaapi.github.io/"><h4 class="text-center text-md-start" >Jan Hajič</h4></a>
                  <p class="text-center text-md-start"><small>Professor at Charles University</small></p>
                  <p style="text-align: justify"><b>Bio:</b> Jan Hajič is a professor of Computational Linguistics at the Institute of Formal and Applied Linguistics, School of Computer Science, Charles University, Prague, Czechia. His interests span fundamental formal linguistic problems, machine translation, deep language understanding, and applications. He has built resources for many languages with rich linguistic annotation, such as the Prague Dependency Treebank; he is currently leading a multi-institutional research infrastructure on language resources in Czechia, LINDAT/CLARIAH-CZ, and coordinating a Horizon Europe pilot project on building LLMs, HPLT. His work experience includes both industrial research (IBM Research Yorktown Heights, NY, USA) and academia (Charles University, Prague, Czechia, Johns Hopkins University and University of Colorado, USA, Fellow of the Centre for Advanced Studies at the Norway Academy of Sciences, and others). He has published more than 200 papers. He is a chair or member of many international and national boards and committees, such as the Steering Committee of the TACL journal.
                  </p>
                  <p><b>Title:</b> <u class="fw-normal">LLMs and Symbolic Meaning Representations</u></p>
                  <p style="text-align: justify"><b>Abstract:</b> LLMs are becoming the mainstream tool for many tasks formerly built under the Natural Language Processing heading, including not only applications such as conversational agents and dialog systems, but also in information retrieval, summarisation, machine translation and others. Yet there is still ongoing research on deep computational linguistics topic, now primarily focused on linguistics as such. However, perhaps there could be still synergies between the linguistically oriented research and the standard LLM and/or end-to-end deep learning approach to improve, for example, LLMs for low-resourced languages as well as for complementing current LLM-based applications with explanatory power or interpretation. In the talk, I will present current developments in LLM building in Europe, show recent developments on the linguistic front (semantic, or meaning representations such as PDT and UMR, and the use of eventive ontologies), and conclude with a list of yet unanswered research questions.
                  </p>
              </div>
          </div>
      </div>



      <div class="container" style="margin-top: 40px">

          <div class="row py-4 px-lg-3 px-0">
              <div class="col-12 col-md-4 col-lg-2 mx-lg-2 order-first" style="display: flex;flex-direction: row;flex-wrap: nowrap;align-content: flex-start;justify-content: center;">
                  <div style="display: flex;justify-content: center;flex-direction: column;">
                      <a href="https://blender.cs.illinois.edu/hengji.html"><img src="https://blender.cs.illinois.edu/HengJi_3.JPG" class="d-block m-2 rounded card-img-top" loading="lazy" style="width:250px;"></a>
                  </div>
              </div>
              <div class="col mx-5">
                  <a href="https://blender.cs.illinois.edu/hengji.html"><h4 class="text-center text-md-start" >Heng Ji</h4></a>
                  <p class="text-center text-md-start"><small>Professor at University of Illinois Urbana-Champaign</small></p>
                  <p style="text-align: justify"><b>Bio:</b> 
			  Heng Ji is a Professor of Computer Science at Siebel School of Computing and Data Science, and a faculty member affiliated with Electrical and Computer Engineering Department, Coordinated Science Laboratory, and Carl R. Woese Institute for Genomic Biology of University of Illinois Urbana-Champaign. She is an Amazon Scholar. She is the Founding Director of Amazon-Illinois Center on AI for Interactive Conversational Experiences (AICE), and the Founding Director of CapitalOne-Illinois Center on AI Safety and Knowledge Systems (ASKS). She received Ph.D. in Computer Science from New York University. Her research interests focus on Natural Language Processing, especially on Multimedia Multilingual Information Extraction, Knowledge-enhanced Large Language Models and Vision-Language Models, and AI for Science. The awards she received include Outstanding Paper Award at ACL2024, two Outstanding Paper Awards at NAACL2024, "Young Scientist" by the World Laureates Association in 2023 and 2024, "Young Scientist" and a member of the Global Future Council on the Future of Computing by the World Economic Forum in 2016 and 2017, "Women Leaders of Conversational AI" (Class of 2023) by Project Voice, "AI's 10 to Watch" Award by IEEE Intelligent Systems in 2013, NSF CAREER award in 2009, PACLIC2012 Best paper runner-up, "Best of ICDM2013" paper award, "Best of SDM2013" paper award, ACL2018 Best Demo paper nomination, ACL2020 Best Demo Paper Award, NAACL2021 Best Demo Paper Award, Google Research Award in 2009 and 2014, IBM Watson Faculty Award in 2012 and 2014 and Bosch Research Award in 2014-2018. She served as the associate editor for IEEE/ACM Transaction on Audio, Speech, and Language Processing, and the Program Committee Co-Chair of many conferences including NAACL-HLT2018 and AACL-IJCNLP2022. She was elected as the North American Chapter of the Association for Computational Linguistics (NAACL) secretary 2020-2023.
                  </p>
                  <p><b>Title:</b> <u class="fw-normal"> Structure is Key to Chemical Language Modeling</u></p>
                  <p style="text-align: justify"><b>Abstract:</b> 
			  Everything in our wonderful world is composed of molecules. Recent advances in block  chemistry involve the manual design of drugs and materials by decomposing molecules into graph substructures—i.e., functional modules—and reassembling them into new molecules with desired functions. However, the process of discovering and manufacturing functional molecules has remained highly artisanal, slow, and expensive. In this talk I will present our recent efforts at teaching computers to speak two complementary languages: one that represents molecular subgraph structures indicative of specific functions, and another that describes these functions in natural language. Unlike existing approaches that add such knowledge as a post hoc step, we developed a function- and synthesis-aware modular chemical language model (mCLM). Inspired by bilingual speakers who frequently “code-switch” (naturally and often switch between their two languages within the same message), we propose a novel neural encoder that integrates molecular structure and natural language. mCLM incorporates both function- and synthesis-related knowledge into the small molecule tokenization process a priori. In experiments on 430 FDA-approved drugs, we find mCLM capable of significantly improving 5 out of 6 chemical functions critical to determining drug potentials. More importantly, mCLM can reason on multiple functions and improve the FDA-rejected drugs (“fallen angels”) over multiple iterations to greatly improve their shortcomings. 
                  </p>
              </div>
          </div>
      </div>

      <div class="container">

          <div class="row py-4 px-lg-3 px-0">
              <div class="col-12 col-md-4 col-lg-2 mx-lg-2 order-first" style="display: flex;flex-direction: row;flex-wrap: nowrap;align-content: flex-start;justify-content: center;">
                  <div style="display: flex;justify-content: center;flex-direction: column;">
                      <a href="https://www.cs.brandeis.edu/~xuen/"><img src="./assets/img/speaker/nianwen-xue.jpg" class="d-block m-2 rounded card-img-top" loading="lazy" style="width:250px;"></a>
                  </div>
              </div>
              <div class="col mx-5">
                  <a href="https://www.cs.brandeis.edu/~xuen/"><h4 class="text-center text-md-start">Nianwen Xue</h4></a>
                  <p class="text-center text-md-start"><small>Professor at Brandeis University</small></p>
                  <p style="text-align: justify"><b>Bio:</b> 
			  Nianwen Xue is a Professor of Linguistics and Computer Science at Brandeis University, specializing in computational linguistics and natural language processing. His research focuses on machine learning methods for syntactic, semantic, and discourse parsing, as well as the creation of large-scale linguistically annotated resources. He has been a principal developer of widely-used linguistic resources, including the Chinese Treebank and the Chinese Proposition Bank, and currently leads the Uniform Meaning Representation (UMR) project, a major initiative dedicated to developing a standardized, cross-linguistic framework for semantic representation. Recently, his research interests have expanded into computational social science, particularly exploring automatic verification of LLM-generated content and the computational analysis of media framing. Xue has served in various editorial and organizational capacities, including as Editor-in-Chief of ACM Transactions on Asian and Low-Resource Language Information Processing (TALLIP) from 2016 to 2019, and as program co-chair for LREC-COLING 2024, a leading international conference in computational linguistics. He currently serves on the editorial boards of Computational Linguistics and the Journal of Language Resources and Evaluation. His research has received support from organizations including the National Science Foundation, DARPA, IARPA, and Amazon Machine Learning Research.
                  </p>
                  <p><b>Title:</b> <u class="fw-normal">Beyond Sentence-Level Semantics: Parsing and Evaluating Document-Level Meaning</u></p>
                  <p style="text-align: justify"><b>Abstract:</b>
			  In the era of Large Language Models (LLMs), the role of linguistic structures in Natural Language Processing (NLP) has fundamentally shifted. Traditionally, linguistic representations such as syntactic trees and semantic graphs served primarily as intermediate forms supporting downstream applications like machine translation and question-answering systems. However, with the advent of LLMs capable of performing many NLP tasks end-to-end, this narrative needs reevaluation. For numerous applications, explicit linguistic structures have become unnecessary, while for others, their use has transitioned from intermediate representations toward end-products.

In this talk, I will introduce Uniform Meaning Representation (UMR), a comprehensive, document-level semantic representation designed to function effectively as a knowledge graph. UMR integrates sentence-level semantic analyses focusing on named entities and predicate-argument structures with document-level analyses that address temporal relations between events and time expressions, modal dependencies involving events and their cognizers, and coreference relations linking entities and events across text. Additionally, I will present preliminary results for parsing UMR structures from English and Chinese texts, along with novel metrics specifically designed for evaluating document-level semantic representations like UMR.
                  </p>
              </div>
          </div>
      </div>



<!--       <div class="container">
          <div class="row py-4 px-lg-3 px-0">
              <div class="col-12 col-md-4 col-lg-2 mx-lg-2 order-first" style="display: flex;flex-direction: row;flex-wrap: nowrap;align-content: flex-start;justify-content: center;">
                  <div style="display: flex;justify-content: center;flex-direction: column;">
                      <a href="https://scottyih.org/"><img src="./assets/img/speaker/Scott-Yih.jpg" class="d-block m-2 rounded card-img-top" loading="lazy" style="width:250px;"></a>
                  </div>
              </div>
              <div class="col mx-5">
                  <a href="https://scottyih.org/"><h4 class="text-center text-md-start">Scott Yih</h4></a>
                  <p class="text-center text-md-start"><small>Scientist at Facebook AI Research (FAIR)</small></p>
                  <p style="text-align: justify"><b>Bio:</b>
                      Scott Wen-tau Yih is a Research Scientist at Facebook AI Research (FAIR).
                      He was elected as ACL Fellow in 2024.
                      His research interests include natural language processing, machine learning and information retrieval.
                      His work on joint inference using integer linear programming (ILP) has been widely adopted in the NLP community for numerous structured prediction problems.
                      Before joining FAIR, Yih was a Principal Research Scientist at the Allen Institute for Artificial Intelligence (AI2), working on scientific question answering.
                      Prior to that, Yih had spent 12 years at Microsoft Research, working on a variety of projects including email spam filtering, keyword extraction and search & ad relevance.
                      His recent work focuses on continuous representations and neural network models, with applications in knowledge base embedding, semantic parsing and question answering.
                      Yih received the best paper award from CoNLL-2011, an outstanding paper award from ACL-2015 and has served as area co-chairs (HLT-NAACL-12, ACL-14, EMNLP-16,17,18),
                      program co-chairs (CEAS-09, CoNLL-14) and action/associated editors (TACL, JAIR) in recent years.
                  </p>
                  <p><b>Title:</b> <u class="fw-normal">[TDB]</u></p>
                  <p style="text-align: justify"><b>Abstract:</b> [TDB]
                  </p>
              </div>
          </div>
      </div> -->





<!--      <div class="container">-->

<!--          <div class="row py-4 px-lg-3 px-0">-->
<!--              <div class="col-12 col-md-4 col-lg-2 mx-lg-2 order-first" style="display: flex;flex-direction: row;flex-wrap: nowrap;align-content: flex-start;justify-content: center;">-->
<!--                  <div style="display: flex;justify-content: center;flex-direction: column;">-->
<!--                      <a href="#"><img src="./assets/img/team/team-2.jpg" class="d-block m-2 rounded card-img-top" loading="lazy" style="width:250px;"></a>-->
<!--                  </div>-->
<!--              </div>-->
<!--              <div class="col mx-5">-->
<!--                  <a href="#"><h4 class="text-center text-md-start">TBD</h4></a>-->
<!--                  <p class="text-center text-md-start"><small>TBD</small></p>-->
<!--                  <p style="text-align: justify"><b>Bio:</b> One of the frequent points in the mainstream narrative about large language-->
<!--                      models is that they have emergent properties", but there is a lot of disagreement about what-->
<!--                      that even means. If they are understood as a kind of generalization beyond training data- as-->
<!--                      something that a model does without being explicitly trained for it- I argue that we have-->
<!--                      not in fact established the existence of any such properties.-->
<!--                  </p>-->
<!--                  <p><b>Title:</b> <u class="fw-normal"> TBD</u></p>-->
<!--                  <p style="text-align: justify"><b>Abstract:</b> TBD.-->
<!--                  </p>-->
<!--              </div>-->
<!--          </div>-->
<!--      </div>-->







<!--      <div class="container" style="margin-top: 40px">-->

<!--          <div class="row py-4 px-lg-3 px-0">-->
<!--              <div class="col-12 col-md-4 col-lg-2 mx-lg-2 order-first" style="display: flex;flex-direction: row;flex-wrap: nowrap;align-content: flex-start;justify-content: center;">-->
<!--                  <div style="display: flex;justify-content: center;flex-direction: column;">-->
<!--                      <img src="./assets/img/team/team-3.jpg" class="d-block m-2 rounded card-img-top" loading="lazy" style="width:250px;">-->
<!--                  </div>-->
<!--              </div>-->
<!--              <div class="col mx-5">-->
<!--                  <h4 class="text-center text-md-start">Anna Rogers</h4>-->
<!--                  <p class="text-center text-md-start"><small>Associate Professor at IT University of-->
<!--                          Copenhagen</small></p>-->
<!--                  <p><b>Bio:</b> One of the frequent points in the mainstream narrative about large language-->
<!--                      models is that they have emergent properties", but there is a lot of disagreement about what-->
<!--                      that even means. If they are understood as a kind of generalization beyond training data- as-->
<!--                      something that a model does without being explicitly trained for it- I argue that we have-->
<!--                      not in fact established the existence of any such properties.-->
<!--                  </p>-->
<!--                  <p><b>Title:</b> <u class="fw-normal">A Sanity Check on Emergent Properties.</u></p>-->
<!--                  <p><b>Abstract:</b> One of the frequent points in the mainstream narrative about large language-->
<!--                      models is that they have emergent properties", but there is a lot of disagreement about what-->
<!--                      that even means. If they are understood as a kind of generalization beyond training data- as-->
<!--                      something that a model does without being explicitly trained for it- I argue that we have-->
<!--                      not in fact established the existence of any such properties, and at the moment we do not-->
<!--                      even have the methodology for doing so.-->
<!--                  </p>-->
<!--              </div>-->
<!--          </div>-->
<!--      </div>-->




<!--      <div class="container" style="margin-top: 40px">-->

<!--          <div class="row py-4 px-lg-3 px-0">-->
<!--              <div class="col-12 col-md-4 col-lg-2 mx-lg-2 order-first" style="display: flex;flex-direction: row;flex-wrap: nowrap;align-content: flex-start;justify-content: center;">-->
<!--                  <div style="display: flex;justify-content: center;flex-direction: column;">-->
<!--                      <img src="./assets/img/team/team-4.jpg" class="d-block m-2 rounded card-img-top" loading="lazy" style="width:250px;">-->
<!--                  </div>-->
<!--              </div>-->
<!--              <div class="col mx-5">-->
<!--                  <h4 class="text-center text-md-start">Anna Rogers</h4>-->
<!--                  <p class="text-center text-md-start"><small>Associate Professor at IT University of-->
<!--                          Copenhagen</small></p>-->
<!--                  <p><b>Bio:</b> One of the frequent points in the mainstream narrative about large language-->
<!--                      models is that they have emergent properties", but there is a lot of disagreement about what-->
<!--                      that even means. If they are understood as a kind of generalization beyond training data- as-->
<!--                      something that a model does without being explicitly trained for it- I argue that we have-->
<!--                      not in fact established the existence of any such properties.-->
<!--                  </p>-->
<!--                  <p><b>Title:</b> <u class="fw-normal">A Sanity Check on Emergent Properties.</u></p>-->
<!--                  <p><b>Abstract:</b> One of the frequent points in the mainstream narrative about large language-->
<!--                      models is that they have emergent properties", but there is a lot of disagreement about what-->
<!--                      that even means. If they are understood as a kind of generalization beyond training data- as-->
<!--                      something that a model does without being explicitly trained for it- I argue that we have-->
<!--                      not in fact established the existence of any such properties, and at the moment we do not-->
<!--                      even have the methodology for doing so.-->
<!--                  </p>-->
<!--              </div>-->
<!--          </div>-->
<!--      </div>-->




    </section>

















    <!-- Pricing Section -->
    <section id="schedule" class="schedule section">

      <!-- Section Title -->
      <div class="container section-title" data-aos="fade-up">
        <h2>Program Schedule</h2>
<!--        <p>Necessitatibus eius consequatur ex aliquid fuga eum quidem sint consectetur velit</p>-->
      </div><!-- End Section Title -->



      <div class="container">

<!--           <p style="color: red;font-size: large;font-weight: bold;">- TBD -</p> -->


       <p>The workshop will be held on Friday, August 1, 2025, which is located in <b style="color: #dc3545">Room 1.61-62</b> at the ACL2025 conference venue.</p>

       <p>Note: each oral presentation will have 10 minutes, including any QA; each keynote talk will have 45 minutes, including any QA.
       </p>
	      
       <p>The schedule for the workshop is the following:
       </p>

       <div class="pt-2 row">
                   <div class="col-12">
                       <table class="table table-borderless">
				<tbody>
				    <tr>
				        <th style="width: 125px;">08:50 - 09:00</th>
				        <td>Opening Remarks</td>
				    </tr>
				    <tr>
				        <th>09:00 - 10:30</th>
				        <td><b>Keynote Session - I </b></td>
				    </tr>
				    <tr><th></th>
				        <td><span class="fw-normal"> The Changing Roles of (Linguistic) Structure in Computational Linguistics</span><br><small>Mark Johnson</small></td>
				    </tr>
				    <tr><th></th>
				        <td><span class="fw-normal">LLMs and Symbolic Meaning Representations</span><br><small>Jan Hajič</small></td>
				    </tr>
				    <tr>
				        <th>10:30 - 11:00</th>
				        <td><i>Coffee break</i></td>
				    </tr>
				    <tr>
				        <th>11:00 - 12:00</th>
				        <td><b>Oral Session - Regular Papers</b></td>
				    </tr>
				    <tr><th></th>
				        <td><span class="fw-normal">BARTABSA++: Revisiting BARTABSA with Decoder LLMs</span><br><small>Jan et al.</small></td>
				    </tr>
				    <tr><th></th>
				        <td><span class="fw-normal">Detecting Referring Expressions in Visually Grounded Dialogue with Autoregressive Language Models</span><br><small>Bram et al.</small></td>
				    </tr>
				    <tr><th></th>
				        <td><span class="fw-normal">Can LLMs Interpret and Leverage Structured Linguistic Representations? A Case Study with AMRs</span><br><small>Ankush et al.</small></td>
				    </tr>
				    <tr><th></th>
				        <td><span class="fw-normal">LLM Dependency Parsing with In-Context Rules</span><br><small>Michael et al.</small></td>
				    </tr>
				    <tr><th></th>
				        <td><span class="fw-normal">From Syntax to Semantics: Evaluating the Impact of Linguistic Structures on LLM-Based Information Extraction</span><br><small>Anushka et al.</small></td>
				    </tr>
				    <tr><th></th>
				        <td><span class="fw-normal">Cross-Document Event-Keyed Summarization</span><br><small>William et al.</small></td>
				    </tr>
				    <tr>
				        <th>12:00 - 12:30</th>
				        <td><b>Poster Session - I </b></td>
				    </tr>
				    <tr>
				        <th>12:30 - 14:00</th>
				        <td><i>Lunch Break</i></td>
				    </tr>
				    <tr>
				        <th>14:00 - 14:45</th>
				        <td><b>Keynote Session - II </b></td>
				    </tr>
				    <tr><th></th>
				        <td><span class="fw-normal">Beyond Sentence-Level Semantics: Parsing and Evaluating Document-Level Meaning</span><br><small>Nianwen Xue</small></td>
				    </tr>
				    <tr>
				        <th>14:45 - 15:30</th>
				        <td><b>Oral Session - Shared Tasks</b></td>
				    </tr>
				    <tr><th></th>
				        <td><span class="fw-normal">DiaDP@XLLM25: Advancing Chinese Dialogue Parsing via Unified Pretrained Language Models and Biaffine Dependency Scoring</span><br><small>Shuoqiu et al.</small></td>
				    </tr>
				    <tr><th></th>
				        <td><span class="fw-normal">SpeechEE@XLLM25: End-to-End Structured Event Extraction from Speech</span><br><small>Soham et al.</small></td>
				    </tr>
				    <tr><th></th>
				        <td><span class="fw-normal">DocIE@XLLM25: UIEPrompter: A Unified Training-Free Framework for Universal Document-Level Information Extraction via Structured Prompt</span><br><small>Chengfeng et al.</small></td>
				    </tr>
				    <tr><th></th>
				        <td><span class="fw-normal">LLMSR@XLLM25: A Language Model-Based Pipeline for Structured Reasoning Data Construction</span><br><small>Hongrui et al.</small></td>
				    </tr>
					
				    <tr>
				        <th>15:30 - 16:00</th>
				        <td><i>Coffee break</i></td>
				    </tr>
				    <tr>
				        <th>16:00 - 16:45</th>
				        <td><b>Keynote Session - III </b></td>
				    </tr>
				    <tr><th></th>
				        <td><span class="fw-normal">Structure is Key to Chemical Language Modeling</span><br><small>Heng Ji</small></td>
				    </tr>
				    <tr>
				        <th>16:45 - 17:00</th>
				        <td>Closing Remarks</td>
				    </tr>
				    <tr>
				        <th>17:00 - 17:30</th>
				        <td><b>Poster Session - II</b></td>
				    </tr>
				</tbody>
			       
                       </table>
                   </div>
               </div>
      </div>


	    
      <div class="container" style="margin-top: 40px">
	      
<h3 style="font-weight: 700; font-size: 32px;">Accepted Papers</h3>
<div class="row gy-4">
<p style="margin-top: 60px;margin-bottom: -10px">We are delighted to congratulate the following papers on being accepted to the XLLM Workshop!
</p>
  
       <p>Oral papers are marked, while the rest are posters.
       </p>
	
<ul style="padding-left: 40px;line-height: 1.8;">
	<li><b  style="font-size: large">Regular workshop papers:</b></li>
	<ul>
		<li>BARTABSA++: Revisiting BARTABSA with Decoder LLMs <b><span style="color:red">(Oral)</span></b></li>
		<li>Detecting Referring Expressions in Visually Grounded Dialogue with Autoregressive Language Models <b><span style="color:red">(Oral)</span></b></li>
		<li>Can LLMs Interpret and Leverage Structured Linguistic Representations? A Case Study with AMRs <b><span style="color:red">(Oral)</span></b></li>
		<li>LLM Dependency Parsing with In-Context Rules <b><span style="color:red">(Oral)</span></b></li>
		<li>From Syntax to Semantics: Evaluating the Impact of Linguistic Structures on LLM-Based Information Extraction <b><span style="color:red">(Oral)</span></b></li>
		<li>Cross-Document Event-Keyed Summarization <b><span style="color:red">(Oral)</span></b></li>
		<li>Fine-Tuning Large Language Models for Relation Extraction within a Retrieval-Augmented Generation Framework</li>
		<li>Benchmarking Table Extraction: Multimodal LLMs vs Traditional OCR</li>
		<li>Injecting Structured Knowledge into LLMs via Graph Neural Networks</li>
		<li>Regular-pattern-sensitive CRFs for Distant Label Interactions</li>
		<li>Exploring Multilingual Probing in Large Language Models: A Cross-Language Analysis</li>
		<li>Self-Contrastive Loop of Thought Method for Text-to-SQL Based on Large Language Model</li>
		<li>Combining Automated and Manual Data for Effective Downstream Fine-Tuning of Transformers for Low-Resource Language Applications</li>
		<li>Seamlessly Integrating Tree-Based Positional Embeddings into Transformer Models for Source Code Representation</li>
		<li>Enhancing AMR Parsing with Group Relative Policy Optimization</li>
		<li>Structure Modeling Approach for UD Parsing of Historical Modern Japanese</li>
		<li>Typed-RAG: Type-Aware Decomposition of Non-Factoid Questions for Retrieval-Augmented Generation</li>
		<li>Do we still need Human Annotators? Prompting Large Language Models for Aspect Sentiment Quad Prediction</li>
		<li>Cognitive Mirroring for DocRE: A Self-Supervised Iterative Reflection Framework with Triplet-Centric Explicit and Implicit Feedback</li>
		<li>Transfer of Structural Knowledge from Synthetic Languages</li>
		<li>Language Models are Universal Embedders</li>
	</ul>

	
	<li style="margin-top: 20px"><b  style="font-size: large">Shared task papers:</b></li>
	<ul>
		<li>DiaDP@XLLM25: Advancing Chinese Dialogue Parsing via Unified Pretrained Language Models and Biaffine Dependency Scoring <b><span style="color:red">(Oral)</span></b></li>
		<li>SpeechEE@XLLM25: End-to-End Structured Event Extraction from Speech <b><span style="color:red">(Oral)</span></b></li>
		<li>SpeechEE@XLLM25: Retrieval-Enhanced Few-Shot Prompting for Speech Event Extraction</li>
		<li>DocIE@XLLM25: UIEPrompter: A Unified Training-Free Framework for Universal Document-Level Information Extraction via Structured Prompt <b><span style="color:red">(Oral)</span></b></li>
		<li>DocIE@XLLM25: In-Context Learning for Information Extraction using Fully Synthetic Demonstrations</li>
		<li>DocIE@XLLM25: ZeroSemble - Robust and Efficient Zero-Shot Document Information Extraction with Heterogeneous Large Language Model Ensembles</li>
		<li>LLMSR@XLLM25: A Language Model-Based Pipeline for Structured Reasoning Data Construction <b><span style="color:red">(Oral)</span></b></li>
		<li>LLMSR@XLLM25: SWRV: Empowering Self-Verification of Small Language Models through Step-wise Reasoning and Verification</li>
		<li>LLMSR@XLLM25: Integrating Reasoning Prompt Strategies with Structural Prompt Formats for Enhanced Logical Inference</li>
		<li>LLMSR@XLLM25: Less is More: Enhancing Structured Multi-Agent Reasoning via Quality-Guided Distillation</li>
		<li>LLMSR@XLLM25: An Empirical Study of LLM for Structural Reasoning</li>
	</ul>
	
	<li style="margin-top: 20px"><b  style="font-size: large">Non-archival extended abstract papers:</b></li>
	<ul>
		<li>Generative Pretrained Structured Transformers: Unsupervised Syntactic Language Models at Scale</li>
		<li>Structured Discourse Representation for Factual Consistency Verification</li>
		<li>Probabilistic Transformer: A Probabilistic Dependency Model for Contextual Word Representation</li>
		<li>Dependency Transformer Grammars: Integrating Dependency Structures into Transformer Language Models</li>
		<li>A Systematic Study of Compositional Syntactic Transformer Language Models</li>
		<li>PICLe: Pseudo-Annotations for In-Context Learning in Low-Resource Named Entity Detection</li>
		<li>LinguaLens: Towards Interpreting Linguistic Structure of Large Language Models via Sparse Auto-Encoder</li>
		<li>Focus on the Emotional Arc! Fine-tune the Model for Ultra-Long Novel Outline Reconstruction</li>
		<li>Scaling Laws and Structure Acquisition in Neural Language Models: A Theory Based on Hierarchical Grammars</li>
		<li>Compositional Generalization and Creativity in Language Diffusion Models</li>
		<li>Leveraging Large Language Models for Structured Sentiment Analysis in Low-resource Domains</li>
		<li>Leveraging LLM-based sentiment analysis for portfolio optimization with proximal policy optimization</li>
	</ul>
</ul>



        </div>


<!--      <div class="container" data-aos="zoom-in" data-aos-delay="100">-->

<!--        <div class="row g-4">-->

<!--          <div class="col-lg-4">-->
<!--            <div class="pricing-item">-->
<!--              <h3>Free Plan</h3>-->
<!--              <div class="icon">-->
<!--                <i class="bi bi-box"></i>-->
<!--              </div>-->
<!--              <h4><sup>$</sup>0<span> / month</span></h4>-->
<!--              <ul>-->
<!--                <li><i class="bi bi-check"></i> <span>Quam adipiscing vitae proin</span></li>-->
<!--                <li><i class="bi bi-check"></i> <span>Nec feugiat nisl pretium</span></li>-->
<!--                <li><i class="bi bi-check"></i> <span>Nulla at volutpat diam uteera</span></li>-->
<!--                <li class="na"><i class="bi bi-x"></i> <span>Pharetra massa massa ultricies</span></li>-->
<!--                <li class="na"><i class="bi bi-x"></i> <span>Massa ultricies mi quis hendrerit</span></li>-->
<!--              </ul>-->
<!--              <div class="text-center"><a href="#" class="buy-btn">Buy Now</a></div>-->
<!--            </div>-->
<!--          </div>&lt;!&ndash; End Pricing Item &ndash;&gt;-->

<!--          <div class="col-lg-4">-->
<!--            <div class="pricing-item featured">-->
<!--              <h3>Business Plan</h3>-->
<!--              <div class="icon">-->
<!--                <i class="bi bi-rocket"></i>-->
<!--              </div>-->

<!--              <h4><sup>$</sup>29<span> / month</span></h4>-->
<!--              <ul>-->
<!--                <li><i class="bi bi-check"></i> <span>Quam adipiscing vitae proin</span></li>-->
<!--                <li><i class="bi bi-check"></i> <span>Nec feugiat nisl pretium</span></li>-->
<!--                <li><i class="bi bi-check"></i> <span>Nulla at volutpat diam uteera</span></li>-->
<!--                <li><i class="bi bi-check"></i> <span>Pharetra massa massa ultricies</span></li>-->
<!--                <li><i class="bi bi-check"></i> <span>Massa ultricies mi quis hendrerit</span></li>-->
<!--              </ul>-->
<!--              <div class="text-center"><a href="#" class="buy-btn">Buy Now</a></div>-->
<!--            </div>-->
<!--          </div>&lt;!&ndash; End Pricing Item &ndash;&gt;-->

<!--          <div class="col-lg-4">-->
<!--            <div class="pricing-item">-->
<!--              <h3>Developer Plan</h3>-->
<!--              <div class="icon">-->
<!--                <i class="bi bi-send"></i>-->
<!--              </div>-->
<!--              <h4><sup>$</sup>49<span> / month</span></h4>-->
<!--              <ul>-->
<!--                <li><i class="bi bi-check"></i> <span>Quam adipiscing vitae proin</span></li>-->
<!--                <li><i class="bi bi-check"></i> <span>Nec feugiat nisl pretium</span></li>-->
<!--                <li><i class="bi bi-check"></i> <span>Nulla at volutpat diam uteera</span></li>-->
<!--                <li><i class="bi bi-check"></i> <span>Pharetra massa massa ultricies</span></li>-->
<!--                <li><i class="bi bi-check"></i> <span>Massa ultricies mi quis hendrerit</span></li>-->
<!--              </ul>-->
<!--              <div class="text-center"><a href="#" class="buy-btn">Buy Now</a></div>-->
<!--            </div>-->
<!--          </div>&lt;!&ndash; End Pricing Item &ndash;&gt;-->

<!--        </div>-->

<!--      </div>-->

    </section><!-- /Pricing Section -->










    <!-- Portfolio Section -->
    <section id="task" class="task section">

      <!-- Section Title -->
      <div class="container section-title" data-aos="fade-up">
        <h2>Shared Tasks</h2>
      </div><!-- End Section Title -->

      <div class="container">

        <p>In addition to paper contributions, we are organizing open challenges on structure-related NLP tasks. Through these shared tasks, we aim to provide a centralized platform for further exploring and advancing traditional or newly-emerged structure-aware tasks.</p>

<!--        <p>We set up three shared tasks:</p>-->
<!--        <ul style="padding-left: 40px;">-->
<!--            <li><b>Task-I:</b> Dialogue-Level Dependency Parsing</li>-->
<!--            <li><b>Task-II:</b> Speech Event Extraction</li>-->
<!--            <li><b>Task-III:</b> Grounded Multimodal Universal Information Extraction</li>-->
<!--            <li><b>Task-IV:</b> xx</li>-->
<!--        </ul>-->

        <p>We have set up three shared tasks as follows.
            Participants can access the respective task pages to learn about the specific participation requirements.
            System submissions will be evaluated using automatic metrics, with a focus on the accuracy and relevance of the results.
            Participants can submit at <a href="https://www.codabench.org/competitions/xxx/">Codabench</a>.
        </p>

        <p>Teams that achieve top rankings in part of the shared tasks will receive cash prizes.
        Winning participants are required to write a technical paper that fully describes the techniques and experimental results used. Additionally, they will need to prepare a poster or oral presentation to showcase their methods and approaches on-site.
        </p>

      </div>



      <div class="container" style="margin-top: 40px">

        <h3 style="font-weight: 700; font-size: 32px;">Task-I: Dialogue-Level Dependency Parsing (DiaDP)</h3>
        <div class="row gy-4">
          <p style="margin-top: 40px;text-align: justify">DiaDP aims to build a unified word-wise dependency tree for dialogue contexts. The tree integrates both inner-EDU dependencies (within Elementary Discourse Units, EDUs) and inter-EDU dependencies (across EDUs) to represent the syntactic and discourse relationships between words in dialogues.
            Given a dialogue consisting of multiple utterances segmented into EDUs, where each utterance is treated as a sentence-like unit, DiaDP outputs a structured dependency tree that includes:
            1) Inner-EDU dependencies: Syntactic relationships within individual EDUs;
            2) Inter-EDU dependencies: Discourse relationships connecting different EDUs, including cross-utterance links.
            We set zero-shot and few-shot learning settings, respectively.
          </p>
          <p style="margin-top: 0px;margin-bottom: -10px;text-align: justify">
              The task bridges the gap between sentence-level dependency parsing and discourse-level parsing by extending syntactic tree structures to dialogue scenarios; incorporating both rhetorical and syntactic elements into the tree.
<!--              We will set <b style="color: #dc3545">cash prizes</b> for -->
              Top-3 teams will receive a certificate for their performance, and will be invited to write technical papers to be included into workshop proceedings.
            For more details to participate, visit the <a href="https://xllms.github.io/DiaDP">DiaDP challenge website</a>.
          </p>

        </div>

      </div>


      <div class="container" style="margin-top: 40px">

        <h3 style="font-weight: 700; font-size: 32px;">Task-II: Speech Event Extraction (SpeechEE)</h3>
        <div class="row gy-4">
          <p style="margin-top: 40px;text-align: justify">
              SpeechEE aims to detect event predicates and arguments directly from audio speech, enabling information acquisition from spoken content such as meetings, interviews, and press releases.
                The SpeechEE is defined as: Given a speech audio input consisting of a sequence of acoustic frames, the goal is to extract structured event records comprising four elements: 1) the event type, 2) the event trigger, 3) event argument roles, and 4) the corresponding event arguments.
          </p>
          <p style="margin-top: 0px;margin-bottom: -10px;text-align: justify">
              This task bridges the gap between traditional textual event extraction and real-world speech scenarios, providing a foundation for structured knowledge extraction from audio data.
<!--              We will set <b style="color: #dc3545">cash prizes</b> for top three teams.-->
              Top-3 teams will receive a certificate for their performance, and will be invited to write technical papers to be included into workshop proceedings.
            For more details to participate, visit the <a href="https://xllms.github.io/SpeechEE">SpeechEE challenge website</a>.
          </p>

        </div>

      </div>



      <div class="container" style="margin-top: 40px">
        <h3 style="font-weight: 700; font-size: 32px;">Task-III: LLM for Structural Reasoning (LLM-SR)</h3>
        <div class="row gy-4">
          <p style="margin-top: 40px;text-align: justify">
              LLM-SR seeks to generate a controllable and interpretable reasoning process by leveraging structural reasoning. LLM-SR requires the structural parsing of two distinct components: major premises and minor premises, then involving identifying fine-grained “alignments” between these two structures and ultimately deriving a conclusion.
          </p>
          <p style="margin-top: 0px;margin-bottom: -10px;text-align: justify">
              This task can be regarded as a constrained Chain-of-Thought (CoT) reasoning process, where reasoning is conducted step by step with reference to facts and relevant rules, thereby improving the transparency and reliability of the process.
              <b style="color: #dc3545">Cash prizes</b> will be awarded to the top three teams.
            For more details to participate, visit the <a href="https://xllms.github.io/LLMSR">LLM-SR challenge website</a>.
          </p>

        </div>

      </div>




      <div class="container" style="margin-top: 40px">
        <h3 style="font-weight: 700; font-size: 32px;">Task-IV: Document-level Information Extraction (DocIE)</h3>
        <div class="row gy-4">
          <p style="margin-top: 40px;text-align: justify">
              DocIE focuses on extracting information from long documents rather than isolated sentences, necessitating the integration of information both within and across multiple sentences while capturing complex interactions. Given a document and a predefined schema, DocIE requires the extraction of each instance (which may be null) corresponding to the schema's elements. This process involves identifying: (1) types of entities, (2) coreference relationships among mentions, (3) types of relations, and (4) the head and tail entities of each identified relation.
          </p>
          <p style="margin-top: 0px;margin-bottom: -10px;text-align: justify">
              This task evaluates the ability of large language models (LLMs) to extract information from long-context documents and comprehend abstract concepts, thereby advancing their application in mining critical, domain-specific information across various fields.
              <b style="color: #dc3545">Cash prizes</b> will be awarded to the top three teams.
            For more details to participate, visit the <a href="https://xllms.github.io/DocIE">DocIE challenge website</a>.
          </p>

        </div>

      </div>



          <div class="container" data-aos="fade-up" style="
            align-content: flex-start;
            display: flex;
            flex-direction: row;
            flex-wrap: nowrap;
            justify-content: center;">
            <div style="
    margin-top: 50px;
">

            <div class="row gy-2" style="
              display: flex;
              justify-content: space-between;
              flex-wrap: nowrap;
              flex-direction: row;">
                <blockquote style="width: fit-content;margin-right: 100px;">
                  <button type="button" class="btn  btn-outline-dark disabled "><a href="https://openreview.net/group?id=aclweb.org/ACL/2025/Workshop/XLLM_Shared_Task"><b class="text-danger">Entry for Shared-Task Paper Submission</b></a></button>
                </blockquote>

            </div>
            </div>
          </div>


    </section><!-- /Portfolio Section -->














    <!-- Team Section -->
    <section id="organizer" class="team section">

      <!-- Section Title -->
      <div class="container section-title" data-aos="fade-up">
        <h2>Organization Team</h2>
<!--        <p>Necessitatibus eius consequatur ex aliquid fuga eum quidem sint consectetur velit</p>-->
      </div><!-- End Section Title -->



      <div class="container">

        <div class="row gy-5">

          <div class="col-lg-4 col-md-6 member" data-aos="fade-up" data-aos-delay="100">
            <div class="member-img">
              <a href="https://haofei.vip/"><img src="https://haofei.vip/images/teampic/feihao-potriat.jpg" class="img-fluid" alt="https://haofei.vip/"></a>
            </div>
            <div class="member-info text-center">
              <a href="https://haofei.vip/"><h4>Hao Fei</h4></a>
              <span>National University of Singapore</span>
<!--              <p>Aliquam iure quaerat voluptatem praesentium possimus unde laudantium vel dolorum distinctio dire flow</p>-->
            </div>
          </div><!-- End Team Member -->

          <div class="col-lg-4 col-md-6 member" data-aos="fade-up" data-aos-delay="200">
            <div class="member-img">
              <a href="https://faculty.sist.shanghaitech.edu.cn/faculty/tukw/"><img src="assets/img/organizer/Kewei.jpg" class="img-fluid" alt="ShanghaiTech University"></a>
            </div>
            <div class="member-info text-center">
              <a href="https://faculty.sist.shanghaitech.edu.cn/faculty/tukw/"><h4>Kewei Tu</h4></a>
              <span>ShanghaiTech University</span>
            </div>
          </div><!-- End Team Member -->

        <div class="col-lg-4 col-md-6 member" data-aos="fade-up" data-aos-delay="200">
          <div class="member-img">
            <a href="https://cs.stanford.edu/~yuhuiz/"><img src="assets/img/organizer/yuhuiz.jpeg" class="img-fluid" alt="ShanghaiTech University"></a>
          </div>
          <div class="member-info text-center">
            <a href="https://cs.stanford.edu/~yuhuiz/"><h4>Yuhui Zhang</h4></a>
            <span>Stanford University</span>
          </div>
        </div><!-- End Team Member -->


          <div class="col-lg-4 col-md-6 member" data-aos="fade-up" data-aos-delay="300">
            <div class="member-img">
            <a href="https://imhuim982.github.io/"><img src="assets/img/organizer/Xiang-Hu.jpg" class="img-fluid" alt="ShanghaiTech University"></a>
            </div>
            <div class="member-info text-center">
            <a href="https://imhuim982.github.io/"><h4>Xiang Hu</h4></a>
              <span>Ant Research</span>
            </div>
          </div><!-- End Team Member -->



          <div class="col-lg-4 col-md-6 member" data-aos="fade-up" data-aos-delay="300">
            <div class="member-img">
            <a href="https://winniehan.github.io/"><img src="assets/img/organizer/Wenjuan.png" class="img-fluid" alt="ShanghaiTech University"></a>
            </div>
            <div class="member-info text-center">
            <a href="https://winniehan.github.io/"><h4>Wenjuan Han</h4></a>
              <span>Beijing Jiaotong University</span>
            </div>
          </div><!-- End Team Member -->





          <div class="col-lg-4 col-md-6 member" data-aos="fade-up" data-aos-delay="300">
            <div class="member-img">
            <a href="https://jzxxx.github.io/"><img src="assets/img/organizer/zixiajia.jpg" class="img-fluid" alt="ShanghaiTech University"></a>
            </div>
            <div class="member-info text-center">
            <a href="https://jzxxx.github.io/"><h4>Zixia Jia</h4></a>
              <span>BigAI</span>
            </div>
          </div>



          <div class="col-lg-4 col-md-6 member" data-aos="fade-up" data-aos-delay="300">
            <div class="member-img">
            <a href="https://zilongzheng.github.io/"><img src="assets/img/organizer/zilong.jpg" class="img-fluid" alt="ShanghaiTech University"></a>
            </div>
            <div class="member-info text-center">
            <a href="https://zilongzheng.github.io/"><h4>Zilong Zheng</h4></a>
              <span>BigAI</span>
            </div>
          </div><!-- End Team Member -->




          <div class="col-lg-4 col-md-6 member" data-aos="fade-up" data-aos-delay="300">
            <div class="member-img">
            <a href="https://sites.google.com/view/yixin-homepage"><img src="assets/img/organizer/caoyixin.png" class="img-fluid" alt="Fudan University"></a>
            </div>
            <div class="member-info text-center">
            <a href="https://sites.google.com/view/yixin-homepage"><h4>Yixin Cao</h4></a>
              <span>Fudan University</span>
            </div>
          </div><!-- End Team Member -->





          <div class="col-lg-4 col-md-6 member" data-aos="fade-up" data-aos-delay="300">
            <div class="member-img">
            <a href="https://zhangmeishan.github.io/"><img src="assets/img/organizer/Meishan.png" class="img-fluid" alt="ShanghaiTech University"></a>
            </div>
            <div class="member-info text-center">
            <a href="https://zhangmeishan.github.io/"><h4>Meishan Zhang</h4></a>
              <span>Harbin Institute of Technology (Shenzhen)</span>
            </div>
          </div><!-- End Team Member -->




          <div class="col-lg-4 col-md-6 member" data-aos="fade-up" data-aos-delay="300">
            <div class="member-img">
            <a href="https://statnlp-research.github.io/"><img src="assets/img/organizer/lu-wei.jpg" class="img-fluid" alt="ShanghaiTech University"></a>
            </div>
            <div class="member-info text-center">
            <a href="https://statnlp-research.github.io/"><h4>Wei Lu</h4></a>
              <span>Singapore University of Technology and Design</span>
            </div>
          </div><!-- End Team Member -->





          <div class="col-lg-4 col-md-6 member" data-aos="fade-up" data-aos-delay="300">
            <div class="member-img">
            <a href="https://homepages.inf.ed.ac.uk/snaraya3/"><img src="https://edinburghnlp.inf.ed.ac.uk/wp-content/uploads/2024/07/sid.png" class="img-fluid" alt="ShanghaiTech University"></a>
            </div>
            <div class="member-info text-center">
            <a href="https://homepages.inf.ed.ac.uk/snaraya3/"><h4>N. Siddharth</h4></a>
              <span>University of Edinburgh</span>
            </div>
          </div><!-- End Team Member -->





          <div class="col-lg-4 col-md-6 member" data-aos="fade-up" data-aos-delay="300">
            <div class="member-img">
            <a href="https://www.mn.uio.no/ifi/english/people/aca/liljao/"><img src="assets/img/organizer/vlilja-ovrelid2.jpg" class="img-fluid" alt="ShanghaiTech University"></a>
            </div>
            <div class="member-info text-center">
            <a href="https://www.mn.uio.no/ifi/english/people/aca/liljao/"><h4>Lilja Øvrelid</h4></a>
              <span>University of Oslo</span>
            </div>
          </div><!-- End Team Member -->





          <div class="col-lg-4 col-md-6 member" data-aos="fade-up" data-aos-delay="300">
            <div class="member-img">
            <a href="https://www.cs.brandeis.edu/~xuen/"><img src="assets/img/organizer/nianwen-xue.jpg" class="img-fluid" alt="ShanghaiTech University"></a>
            </div>
            <div class="member-info text-center">
            <a href="https://www.cs.brandeis.edu/~xuen/"><h4>Nianwen Xue</h4></a>
              <span>Brandeis University</span>
            </div>
          </div><!-- End Team Member -->





          <div class="col-lg-4 col-md-6 member" data-aos="fade-up" data-aos-delay="300">
            <div class="member-img">
            <a href="https://frcchang.github.io/"><img src="assets/img/organizer/Yue-Zhang.jpg" class="img-fluid" alt="Westlake University"></a>
            </div>
            <div class="member-info text-center">
            <a href="https://frcchang.github.io/"><h4>Yue Zhang</h4></a>
              <span>Westlake University</span>
            </div>
          </div><!-- End Team Member -->





<!--          <div class="col-lg-4 col-md-6 member" data-aos="fade-up" data-aos-delay="600">-->
<!--            <div class="member-img">-->
<!--              <img src="assets/img/team/team-6.jpg" class="img-fluid" alt="">-->
<!--              <div class="social">-->
<!--                <a href="#"><i class="bi bi-twitter-x"></i></a>-->
<!--                <a href="#"><i class="bi bi-facebook"></i></a>-->
<!--                <a href="#"><i class="bi bi-instagram"></i></a>-->
<!--                <a href="#"><i class="bi bi-linkedin"></i></a>-->
<!--              </div>-->
<!--            </div>-->
<!--            <div class="member-info text-center">-->
<!--              <h4>Josepha Palas</h4>-->
<!--              <span>Operation</span>-->
<!--              <p>Sint sint eveniet explicabo amet consequatur nesciunt error enim rerum earum et omnis fugit eligendi cupiditate vel</p>-->
<!--            </div>-->
<!--          </div>&lt;!&ndash; End Team Member &ndash;&gt;-->

        </div>

      </div>

    </section><!-- /Team Section -->










    <!-- committee Section -->
    <section id="committee" class="committee section">

      <!-- Section Title -->
      <div class="container section-title" data-aos="fade-up">
        <h2>Program Committee</h2>
<!--        <p>Necessitatibus eius consequatur ex aliquid fuga eum quidem sint consectetur velit</p>-->
      </div><!-- End Section Title -->



<!--      <div class="container">-->
<!--          <p style="color: red;font-size: large;font-weight: bold;">- TBD -</p>-->
<!--      </div>-->


      <div class="container">

        <div class="row gy-2">

          <ul style="padding-left: 40px;line-height: 1.8;">

                <li> <a href="https://www3.nd.edu/~dchiang/">David Chiang</a>, University of Notre Dame </li>
<!--                <li> Yohei Oseki, University of Tokyo </li>-->
                <li> <a href="https://xuanjing-huang.github.io/">Huang Xuanjing</a>, Fudan University</li>
                <li> <a href="https://stanojevic.github.io/">Milos Stanojevic</a>, University College London </li>
                <li> <a href="https://jennhu.github.io/">Jennifer Hu</a>, Harvard University </li>
                <li> <a href="https://www.cylab.cmu.edu/directory/bios/li-lei.html">Lei Li</a>, Carnegie Mellon University </li>
                <li> <a href="https://jnivre.github.io/">Joakim Nivre</a>, Uppsala University </li>
                <li> <a href="https://mariannaapi.github.io/">Marianna Apidianaki</a>, University of Pennsylvania </li>
                <li> <a href="https://nlp.csai.tsinghua.edu.cn/~lzy/">Zhiyuan Liu</a>,  Tsinghua University</li>
                <li> <a href="https://profiles.auckland.ac.nz/liu-qian">Qian Liu</a>, University of Auckland</li>
                <li> <a href="https://pauillac.inria.fr/~seddah/">Djamé Seddah</a>, University Paris-Sorbonne </li>
<!--                <li> <a href="http://hlt.suda.edu.cn/~zhli/en.html">Zhenghua Li</a>, Soochow University</li>-->
<!--                <li> Percy Liang, Stanford University </li>-->
<!--                <li> Dan Roth, University of Pennsylvania </li>-->
<!--                <li> Luke Zettlemoyer, University of Washington </li>-->

                <li> <a href="https://sustcsonglin.github.io/">Songlin Yang</a>, Massachusetts Institute of Technology </li>
                <li> <a href="https://www.emorynlp.org/bachelors/jayeol-chun">Jayeol Chun</a>, Brandeis University</li>
                <li> <a href="https://liziliao.github.io/">Lizi Liao</a>, Singapore Management University</li>
                <li> <a href="https://people.cs.georgetown.edu/nschneid/">Natha Schneider</a>, Georgetown University </li>
                <li> <a href="https://zcli-charlie.github.io/">Zuchao Li</a>, Wuhan University </li>
                <li> <a href="https://attapol.github.io/">Attapol Rutherford</a>, Chulalongkorn University </li>
                <li> <a href="https://xinyadu.github.io/">Xinya Du</a>, University of Texas at Dallas </li>
                <li> <a href="https://cs.uwaterloo.ca/~fhs/">Freda Shi</a>, University of Waterloo</li>

                <li> <a href="https://jflanigan.github.io/">Jeff Flanigan</a>, University of California, Santa Cruz</li>
                <li> <a href="https://people.ucas.ac.cn/~hongyu">Hongyu Lin</a>, Chinese Academy of Sciences </li>
                <li> <a href="https://jinlanfu.github.io/">Jinlan Fu</a>, National University of Singapore</li>
                <li> <a href="https://liangmingpan.bio/">Liangming Pan</a>, University of Arizona </li>


                <li> <a href="https://namednil.github.io/">Matthias Lindemann</a>, University of Edinburgh </li>
                <li> <a href="https://bcmi.sjtu.edu.cn/home/zhangzs/">Zhuosheng Zhang</a>, Shanghai Jiao Tong University</li>
                <li> <a href="https://mopper97.github.io/">Mattia Opper</a>, University of Edinburgh </li>
                <li> <a href="#">Yanpeng Zhao</a>, Beijing Institute for General Artificial Intelligence </li>
                <li> <a href="https://shirawein.github.io/">Shira Wein</a>, Amherst College </li>

                <li> <a href="https://sph.nus.edu.sg/faculty-directory/he-kai/">He Kai</a>, National University of Singapore </li>
                <li> <a href="https://wenyueh.github.io/">Wenyue Hua</a>, University of California, Santa Barbara </li>


                <li> <a href="https://cs.stanford.edu/~anjiang/">Anjiang Wei</a>, Stanford University</li>
                <li> <a href="https://yoshiryo0617.github.io/github-pages/">Ryo Yoshida</a>, University of Tokyo </li>
                <li> <a href="https://lilei-nlp.github.io/">Lei Li</a>, University of Hong Kong</li>
                <li> <a href="https://www.colorado.edu/linguistics/julia-bonn">Julia Bonn</a>, University of Colorado at Boulder</li>

                <li> <a href="#">Chao Lou</a>, ShanghaiTech University</li>
                <li> <a href="#">Haoyi Wu</a>, ShanghaiTech University</li>
                <li> <a href="https://yzhang.site/">Yu Zhang</a>, Soochow University </li>
                <li> <a href="https://sqwu.top/">Shengqiong Wu</a>, National University of Singapore </li>
                <li> <a href="https://jrc1995.github.io/">Jishnu Ray Chowdhury</a>, University of Illinois Chicago </li>


<!--              =========================-->

<!--                <li> Ryan Cotterell, ETH Zürich </li>-->
<!--                <li> Scott Wen-tau Yih, Meta AI </li>-->

<!--                <li> Min-Yen Kan, National University of Singapore </li>-->
<!--                <li> Ryan McDonald, Microsoft Research </li>-->
<!--                <li> Alexander Clark, Gothenburg University </li>-->
<!--                <li> Heng Ji, University of Illinois Urbana-Champaign </li>-->
<!--                <li> Xuanjing Huang, Fudan University </li>-->
<!--                <li> Erik Cambria, Nanyang Technological University </li>-->
<!--                <li> Luheng He, Google </li>-->
<!--                <li> Freda Shi, University of Waterloo </li>-->
<!--                <li> Yikang Shen, MIT-IBM Watson Lab </li>-->
<!--                <li> Jan Hajič, Charles University </li>-->


          </ul>


      </div>

    </section><!-- /committee Section -->
















    <!-- Contact Section -->
    <section id="contact" class="contact section">

      <!-- Section Title -->
      <div class="container section-title" data-aos="fade-up">
        <h2>Contact</h2>
<!--        <p>Necessitatibus eius consequatur ex aliquid fuga eum quidem sint consectetur velit</p>-->
      </div><!-- End Section Title -->

      <div class="container" data-aos="fade-up" data-aos-delay="100">

        <div>
          <h5>Join and post at our <a href="https://groups.google.com/g/xllm2025">Google Group</a>!</h5>
          <h5>Email the organziers at <a href="mailto:xllm2025@googlegroups.com ">xllm2025@googlegroups.com</a>.</h5>

        </div>


      </div>

    </section><!-- /Contact Section -->

  </main>


  <footer id="footer" class="footer position-relative light-background" style="
    padding-bottom: 0;
">


    <div class="container copyright text-center mt-4" style="
    background-color: transparent;
">
      <p>© <span>Copyright</span> <strong class="sitename">Append</strong> <span>All Rights Reserved</span></p>
      <div class="credits">
        <!-- All the links in the footer should remain intact. -->
        <!-- You can delete the links only if you've purchased the pro version. -->
        <!-- Licensing information: https://bootstrapmade.com/license/ -->
        <!-- Purchase the pro version with working PHP/AJAX contact form: [buy-url] -->
        Designed by <a href="https://bootstrapmade.com/">BootstrapMade</a>
      </div>
    </div>

  </footer>

  <!-- Scroll Top -->
  <a href="#" id="scroll-top" class="scroll-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Preloader -->
  <div id="preloader"></div>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>
  <script src="assets/vendor/aos/aos.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
  <script src="assets/vendor/imagesloaded/imagesloaded.pkgd.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>

  <!-- Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>
